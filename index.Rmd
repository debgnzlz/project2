---
title: 'Project 2: Data Mining, Classification, Prediction'
author: "SDS322E"
date: ''
output:
  html_document:
    toc: yes
    toc_float:
      collapsed: no
      smooth_scroll: yes
  pdf_document:
    toc: no
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, fig.align = "center", warning = F, message = F,
tidy=TRUE, tidy.opts=list(width.cutoff=60), R.options=list(max.print=100))

class_diag <- function(score, truth, positive, cutoff=.5){

  pred <- factor(score>cutoff,levels=c("TRUE","FALSE"))
  truth <- factor(truth==positive, levels=c("TRUE","FALSE"))

  tab<-table(truth, pred)
  acc=sum(diag(tab))/sum(tab)
  sens=tab[1,1]/rowSums(tab)[1]
  spec=tab[2,2]/rowSums(tab)[2]
  ppv=tab[1,1]/colSums(tab)[1]

#CALCULATE F1
  f1=2*(sens*ppv)/(sens+ppv)
  
#CALCULATE EXACT AUC
  truth<-as.numeric(truth=="TRUE")
  ord<-order(score, decreasing=TRUE)
  score <- score[ord]; truth <- truth[ord]
  TPR=cumsum(truth)/max(1,sum(truth))
  FPR=cumsum(!truth)/max(1,sum(!truth))
  dup<-c(score[-1]>=score[-length(score)], FALSE)
  TPR<-c(0,TPR[!dup],1); FPR<-c(0,FPR[!dup],1)
  n <- length(TPR)
  auc<- sum( ((TPR[-1]+TPR[-n])/2) * (FPR[-1]-FPR[-n]) )
  round(data.frame(acc,sens,spec,ppv,f1,ba=(sens+spec)/2,auc, row.names = "Metrics"),4)
}
```

# Mining, Classification, Prediction

## Debbie Gonzalez, dg37725

### Introduction 

I'm a huge Taylor Swift fan, so I thought it'd be fun to do this project on data mining, classification, and prediction using her music. I'm using spotify_taylorswift, a data set extracted from Spotify WebAPI last November 6, 2021. I found this data on https://www.kaggle.com/thespacefreak/taylor-swift-spotify-data. There are 171 observations of 16 variables, and there are 171 observations per group for my categorical/binary variables. Some of the variables in this dataset are:

name: Name of song//
album: Name of album//
artist: Name of artist/s involved//
release_date: Release date of album//
length: Song length in milliseconds//
popularity: Percent popularity of the song based on Spotify's algorithm (possibly the number of stream at a certain period of time)//
danceability: How suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity//
acousticness: How acoustic a song is//
energy: A perceptual measure of intensity and activity//
instrumentalness: The amount of vocals in the song//
liveness: Probability that the song was recorded with a live audience//


```{R}
library(tidyverse)
spotify_taylorswift <- read_csv("~/project2/spotify_taylorswift.csv")
spotify_taylorswift$Binary<-ifelse(spotify_taylorswift$popularity>60,"True","False")
```

### Cluster Analysis

```{R}
library(cluster)
# clustering code here

clusterr <- spotify_taylorswift %>% select(popularity, danceability, acousticness)
silwidth <- vector()

for(i in 2:10){
q1<-clusterr%>%daisy("gower")%>%pam(k=i,diss=T)
  silwidth[i]<-q1$silinfo$avg.width
}
ggplot()+geom_path(aes(x=2:10,y=silwidth[2:10]))

cluster_pam <- clusterr %>% pam(k=2)

cluster_pam

cluster_pam$silinfo$avg.width

library(GGally)
clusterr %>% mutate(cluster=as.factor(cluster_pam$clustering)) %>% 
  ggpairs(columns=1:3, aes(color=cluster))

```

Here I performed a cluster analysis by (1) choosing the number of clusters that maximizes average silhouette width (k=2), and (2) running the PAM clustering algorithm on the data for that many clusters. Based on the average silhouette width of 0.6468007, the cluster solution is reasonable. 

I used my data, added the PAM cluster assignments as a factor or character variable, and then plotted every pairwise scatterplot using the `ggpairs(cols= 1:3, aes(color=cluster))` function. Popularity shows the biggest difference between the clusters, and danceability shows the least difference between the clusters. 
    
    
### Dimensionality Reduction with PCA

```{R}
# PCA code here

princomp(clusterr, cor=T) -> pca1

summary(pca1)

eigval<-pca1$sdev^2
varprop=round(eigval/sum(eigval), 2)
round(cumsum(eigval)/sum(eigval), 2)

summary(pca1, loadings=T)

df <- pca1$scores %>% as.data.frame()

df %>% ggplot(aes(x=Comp.1, y=Comp.2)) + geom_point(size=4)
```
The PCs represent the correlation between popularity and danceability and acousticness. Scoring high/low on each of these components means there is either a high/low correlation between popularity and danceability and acousticness. A large number of my total variance in my dataset is explained by these PCs. I notice that if a song has high popularity, it tends to have high PC1 scores

###  Linear Classifier

```{R}
# linear classifier code here

library(knitr)
opts_chunk$set(fig.align="center", fig.height=5, message=FALSE, warning=FALSE, fig.width=8, tidy.opts=list(width.cutoff=60),tidy=TRUE)

class_diag <- function(score, truth, positive, cutoff=.5){

  pred <- factor(score>cutoff,levels=c("TRUE","FALSE"))
  truth <- factor(truth==positive, levels=c("TRUE","FALSE"))

  tab<-table(truth, pred)
  acc=sum(diag(tab))/sum(tab)
  sens=tab[1,1]/rowSums(tab)[1]
  spec=tab[2,2]/rowSums(tab)[2]
  ppv=tab[1,1]/colSums(tab)[1]

#CALCULATE F1
  f1=2*(sens*ppv)/(sens+ppv)
  
#CALCULATE EXACT AUC
  truth<-as.numeric(truth=="TRUE")
  ord<-order(score, decreasing=TRUE)
  score <- score[ord]; truth <- truth[ord]
  TPR=cumsum(truth)/max(1,sum(truth))
  FPR=cumsum(!truth)/max(1,sum(!truth))
  dup<-c(score[-1]>=score[-length(score)], FALSE)
  TPR<-c(0,TPR[!dup],1); FPR<-c(0,FPR[!dup],1)
  n <- length(TPR)
  auc<- sum( ((TPR[-1]+TPR[-n])/2) * (FPR[-1]-FPR[-n]) )
  round(data.frame(acc,sens,spec,ppv,f1,ba=(sens+spec)/2,auc, row.names = "Metrics"),4)
}
  
logistic_fit <- glm(Binary=="True" ~ danceability + acousticness, data=spotify_taylorswift, family="binomial")

prob_reg <- predict(logistic_fit)

class_diag(prob_reg, spotify_taylorswift$Binary, positive="True")


```

```{R}
# cross-validation of linear classifier here

set.seed(322)
k=10

data<-sample_frac(spotify_taylorswift) #randomly order rows
folds <- rep(1:k, length.out=nrow(data)) #create folds

diags<-NULL

i=1
for(i in 1:k){
# create training and test sets
train<-data[folds!=i,] 
test<-data[folds==i,] 
truth<-test$Binary

# train model
fit <- glm(Binary=="True" ~ danceability + acousticness, data = train, family  = "binomial")

# test model
probs <- predict(fit, test)

# get performance metrics for each fold
diagsi <- class_diag(probs, truth, positive = "True")
diags<-rbind(diags,diagsi) }

#average performance metrics across all folds
summarize_all(diags,mean)
```

The model is performing less than good per AUC. I do not see any signs of overfitting (I did not see a noticeable decrease in AUC when predicting out of sample).

### Non-Parametric Classifier

```{R}
library(caret)
# non-parametric classifier code here
attach(spotify_taylorswift)
knn_fit <- knn3(Binary=="True" ~ danceability + acousticness, data=spotify_taylorswift)
prob_reg <- predict(knn_fit)
diag_knn <- class_diag(prob_reg[,2], spotify_taylorswift$Binary, positive = "True")
print(diag_knn)
```

```{R}
# cross-validation of np classifier here
set.seed(322)
k=10
data<-sample_frac(spotify_taylorswift) #randomly order rows
folds <- rep(1:k, length.out=nrow(data)) #create folds

diags<-NULL

i=1
for(i in 1:k){
  #create training and test sets
  train<-data[folds!=i,] 
  test<-data[folds==i,] 
  truth<-test$Binary

# train model
knn_fit <- knn3(Binary=="True" ~ danceability + acousticness, data = train)

# test model
prob_knn <- predict(knn_fit, test)

# get performance metrics for each fold
diags<-rbind(diags,class_diag(prob_knn[,2], truth, positive = "True")) }

#average performance metrics across all folds
summarize_all(diags,mean)
```

The model is performing less than good per AUC. I do see signs of overfitting because I see a real decrease in AUC when predicting out of sample. I believe my previous model preformed better in its cross-validation performance. 


### Regression/Numeric Prediction

```{R}
# regression model code here
fit<-lm(popularity~.,data=clusterr) #predict popularity from all other variables
yhat<-predict(fit) #predicted popularity
mean((clusterr$popularity-yhat)^2) #mean squared error
```

```{R}
# cross-validation of regression model here
set.seed(1234)
k=5 #choose number of folds
data<-clusterr[sample(nrow(clusterr)),] #randomly order rows
folds<-cut(seq(1:nrow(clusterr)),breaks=k,labels=F) #create folds
diags<-NULL
for(i in 1:k){
  train<-data[folds!=i,]
  test<-data[folds==i,]
  ## Fit linear regression model to training set
  fit<-lm(popularity~.,data=train)
  ## Get predictions/y-hats on test set (fold i)
  yhat<-predict(fit,newdata=test)
  ## Compute prediction error  (MSE) for fold i
  diags<-mean((test$popularity-yhat)^2) 
}
mean(diags) ## get average MSE across all folds (much higher error)!
```

I see signs of overfitting because the MSE is higher in cross-validation (138 vs 177). 

### Python 

```{R}
library(reticulate)
grammy <- "Taylor Swift is"
cat(c(grammy,py$grammy))
```

```{python}
# python code here
grammy = "the best!"
print(r.grammy, grammy)
```

The reticulate package includes a Python engine for R Markdown that enables easy interoperability between Python and R; Python and Rhave full access to each otherâ€™s objects. All objects created within Python chunks are available to R using the py (py$) object exported by library(reticulate), similarly, R objects within Python chunks via the r (.r) object.

### Concluding Remarks

I've enjoyed working on this project and learned so much about data mining, classification, and prediction in R. I've also learned Python for data science and I'm excited to continue my knowledge!




